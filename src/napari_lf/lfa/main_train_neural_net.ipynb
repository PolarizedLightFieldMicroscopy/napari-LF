{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network training for Napari-LF\n",
    "\n",
    "Napari-LF neural net integration relies on Pytorch-lightning (PL) workflow. Which provides general functions for loading data, training, inference, etc. That can be used with any neural network.\n",
    "This script is intended for preparing a network to use with Napari-LF.\n",
    "\n",
    "PL provides function hooks for the main tasks required during training/testing a network, functions like:\n",
    "(See LFNeuralNetworksProto.py or VCDNet.py for examples)\n",
    "- configure_optimizers: Create an optimizer and save it to the network object. In our case we use ADAM\n",
    "- training_step / validation_step: These functions are called by the trainer created in step 5. Note that LFMNet and VCDNet use the same training/validation functions, that's why we overload this functions in the base class.\n",
    "\n",
    "\n",
    "## Instructions\n",
    "1. Import required libraries and desired network to train.\n",
    "2. Gather needed information from user\n",
    "3. Create a network.\n",
    "4. Load data for training.\n",
    "5. Train network.\n",
    "6. Use the network to reconstruct a sample\n",
    "\n",
    "**Extra**: Load and continue training a network from a checkpoint. Then evaluate on an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import required libraries and desired network to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "# Let's train a VCDNet. This defines which network we will train\n",
    "# from neural_nets.VCDNet import VCDNet as NN\n",
    "# Or:\n",
    "from neural_nets.LFMNet import LFMNet as NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Gather needed information from user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RO: is n_gpus the number of gpus available?\n",
    "n_gpus = 1\n",
    "# What is the shape of our Light-field [angular-u, angular-v, spatial-s, spatial-t]\n",
    "LFshape = [33,33,39,39]     # For the case of the MouseBrain dataset\n",
    "LF_2D_shape = [LFshape[0]*LFshape[2], LFshape[1]*LFshape[3]]\n",
    "# How many depths are present in each volume?\n",
    "n_depths = 64\n",
    "\n",
    "# Define training parameters\n",
    "training_settings = {}\n",
    "# Learning rate\n",
    "training_settings['lr'] = 1e-3    \n",
    "# Batch size              \n",
    "training_settings['batch_size'] = 2  \n",
    "# max epochs to train        \n",
    "training_settings['epochs'] = 5\n",
    "# Which image-volume pairs to use\n",
    "training_settings['images_ids'] = list(range(10))                                               \n",
    "# Where is the data\n",
    "training_settings['dataset_path'] = 'D:/BrainImagesJosuePage/Brain_40x_64Depths_362imgs.h5'    \n",
    "# Where to store the trained network?\n",
    "# training_settings['output_dir'] = 'C:/Users/OldenbourgLab2/Code/napari-LF-neural_nets/examples/pretrained_networks/'    # If left blank the logs and trained network are stored at ./lightning_logs/version_*\n",
    "training_settings['output_dir'] = 'C:/Users/OldenbourgLab2/Desktop'\n",
    "# Where to store the trained network?\n",
    "training_settings['output_prefix'] = ''    # if left blank incremental versioning will be used 'version_*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a network\n",
    "Neural networks are derived classes based on a common LFNeuralNetworkProto.py and share the same constructor.\n",
    "Hence we can use it with any network (LFMNet, VCDNet, ...) as long as we import it as NN (see import on the top) and its derived from the LFNeuralNetworkProto class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = LF_2D_shape\n",
    "output_shape = (n_depths,)+tuple(LF_2D_shape)\n",
    "# Create our network\n",
    "net = NN(input_shape, output_shape, \n",
    "         network_settings_dict={'LFshape' : LFshape}, \n",
    "         training_settings_dict=training_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Load data for training.\n",
    "The PL trainer object (in the next step), requires a train and validation dataloader.\n",
    "In the configure_dataloader() function, we define the members:\n",
    "- net.train_loader, net.val_loader are of type torch.utils.data.DataLoader and wrap a Dataset object implementing functions like: \\_\\_getitem\\_\\_ and \\_\\_len\\_\\_ (see Dataset in LFUtil.py)\n",
    "- max_LF_train, max_vol_train: Normalization values stored during training, which are used to normalize the data every time a reconstruction is required.\n",
    "\n",
    "Each network should have its custom dataloder. As LFMNet and VCDNet use different shape of LF as inpute, each have their own implementation of configure_dataloder()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████████████████████████████████████████████████████████████| 10/10 [00:10<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "net.configure_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Where to save the logs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################### Logging to: C:/Users/OldenbourgLab2/Desktop/LFMNet\n",
      "run in the terminal to view progress:\n",
      "tensorboard --logdir=C:/Users/OldenbourgLab2/Desktop/LFMNet\\version_2\n"
     ]
    }
   ],
   "source": [
    "# Do we log to the default directory? or to a specified one \n",
    "tb_logger = True\n",
    "output_path = './lightning_logs/'\n",
    "if len(training_settings['output_dir']) > 0: # Do we have a path for the logging?\n",
    "    # Define network type\n",
    "    network_name = net.__class__.__name__\n",
    "    from pytorch_lightning import loggers as pl_loggers\n",
    "    version = None if len(training_settings['output_prefix'])==0 else training_settings['output_prefix']\n",
    "    tb_logger = pl_loggers.TensorBoardLogger(save_dir=f\"{training_settings['output_dir']}/\", name=network_name, version=version)\n",
    "    output_path = f\"{training_settings['output_dir']}/{network_name}\"\n",
    "\n",
    "print(f'###################### Logging to: {output_path}' )\n",
    "print(f'run in the terminal to view progress:')\n",
    "print(f'tensorboard --logdir={tb_logger.log_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2: Train the network\n",
    "For training we use PL trainers, specify which computing devices use and on which data (train/val_dataloaders loaded in step 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type       | Params\n",
      "--------------------------------------------------\n",
      "0 | lensletConvolution | Sequential | 46.7 K\n",
      "1 | Unet               | UNetLF     | 29.2 K\n",
      "--------------------------------------------------\n",
      "75.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "75.9 K    Total params\n",
      "0.304     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OldenbourgLab2\\anaconda3\\envs\\napari-new\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\OldenbourgLab2\\anaconda3\\envs\\napari-new\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\OldenbourgLab2\\anaconda3\\envs\\napari-new\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1892: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be7af4d11d34883be3be68fe749f2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "# Create a trainer\n",
    "trainer = pl.Trainer(logger=tb_logger, accelerator='gpu' if n_gpus>0 else 'cpu', devices=n_gpus, precision=32, max_epochs=net.get_train_setting('epochs'))\n",
    "# Train the network \n",
    "trainer.fit(model=net, train_dataloaders=net.train_loader, val_dataloaders=net.val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Reconstruct a volume with the trained network\n",
    "Lets grab an image from the validation dataloader and predict it's 3D reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 1, 1287, 1287]' is invalid for input of size 2405601",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m pred_vol \u001b[38;5;241m=\u001b[39m net(LF_img_4D\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Compute 2D lenslet image for display\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m LF_img_lenslet \u001b[38;5;241m=\u001b[39m \u001b[43mLF2Spatial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLF_img_4D\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLFshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Compute volume Maximum intensity projection\u001b[39;00m\n\u001b[0;32m     11\u001b[0m vol_MIP \u001b[38;5;241m=\u001b[39m volume_2_projections(pred_vol)\n",
      "File \u001b[1;32m~\\Documents\\GitHub\\napari-LF\\src\\napari_lf\\lfa\\neural_nets\\util\\LFUtil.py:161\u001b[0m, in \u001b[0;36mLF2Spatial\u001b[1;34m(xIn, LFSize)\u001b[0m\n\u001b[0;32m    159\u001b[0m x \u001b[38;5;241m=\u001b[39m xIn\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xIn\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m6\u001b[39m:\n\u001b[1;32m--> 161\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mxIn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxShape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxShape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLFSize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mLFSize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLFSize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mLFSize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xIn\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m    163\u001b[0m     x \u001b[38;5;241m=\u001b[39m xIn\u001b[38;5;241m.\u001b[39mview(xShape[\u001b[38;5;241m0\u001b[39m],xShape[\u001b[38;5;241m1\u001b[39m],LFSize[\u001b[38;5;241m2\u001b[39m],LFSize[\u001b[38;5;241m0\u001b[39m],LFSize[\u001b[38;5;241m3\u001b[39m],LFSize[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m4\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[1, 1, 1287, 1287]' is invalid for input of size 2405601"
     ]
    }
   ],
   "source": [
    "from neural_nets.util.LFUtil import *\n",
    "import matplotlib.pyplot as plt\n",
    "# Grab a sample\n",
    "LF_img_4D, GT_vol = net.val_loader.dataset.__getitem__(0)\n",
    "# perform reconstruction by calling the network with the image (This calls the forward function in the network)\n",
    "pred_vol = net(LF_img_4D.unsqueeze(0))\n",
    "\n",
    "# Compute 2D lenslet image for display\n",
    "LF_img_lenslet = LF2Spatial(LF_img_4D.unsqueeze(0), LFshape)\n",
    "# Compute volume Maximum intensity projection\n",
    "vol_MIP = volume_2_projections(pred_vol)\n",
    "\n",
    "# Show input and output\n",
    "plt.subplot(1,2,1)\n",
    "# Pytorch output dimensions are [batch, channel, x, y]. \n",
    "#We grab only the [x,y] and convert it to cpu and numpy for display\n",
    "plt.imshow(LF_img_lenslet[0,0].cpu().detach().numpy()) \n",
    "plt.title('Input Light-field image')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(vol_MIP[0,0].cpu().detach().numpy()) \n",
    "plt.title('Output 3D volume')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Extra**: Continue training a network from a checkpoint\n",
    "LFNeuralNetworkProto has a function to load networks, which is agnostic to the type of network, as it will extract and import the correct one from the checkpoint information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Extra.1** Load pretrained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_nets.LFNeuralNetworkProto import *\n",
    "# Where is the checkpoint of the network?\n",
    "checkpoint_path = tb_logger.log_dir.replace(\"\\\\\",\"/\").replace(\"//\",\"/\") + '/checkpoints/*.ckpt'\n",
    "# checkpoint_path = 'C:/Users/OldenbourgLab2/Code/napari-LF-neural_nets/examples/pretrained_networks/VCDNet/version_0/checkpoints/*.ckpt'\n",
    "print(\"Loading checkpoint from: \" + checkpoint_path)\n",
    "n_retrain_epochs = 10 # How many epochs to train?\n",
    "# Create network\n",
    "net = LFNeuralNetworkProto.load_network_from_file(checkpoint_path, LFshape)\n",
    "# Load datasets\n",
    "net.configure_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extra.2 Create a new trainer and train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_retrain_epochs > 0:\n",
    "    # Create a trainer\n",
    "    trainer = pl.Trainer(logger=tb_logger, accelerator='gpu' if n_gpus>0 else 'cpu', devices=n_gpus, precision=32, max_epochs=n_retrain_epochs)\n",
    "    # Train the network \n",
    "    trainer.fit(model=net, train_dataloaders=net.train_loader, val_dataloaders=net.val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extra.3 Load a trained network and run inference on a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_nets.util.LFUtil import *\n",
    "import matplotlib.pyplot as plt\n",
    "from neural_nets.LFNeuralNetworkProto import *\n",
    "\n",
    "# Where is the checkpoint of the network?\n",
    "checkpoint_path = 'C:/Users/OldenbourgLab2/Code/napari-LF-neural_nets/examples/pretrained_networks/VCDNet/version_32/checkpoints/*.ckpt'\n",
    "print(\"Loading checkpoint from: \" + checkpoint_path)\n",
    "# Create network\n",
    "net = LFNeuralNetworkProto.load_network_from_file(checkpoint_path, LFshape)\n",
    "# Load datasets\n",
    "net.configure_dataloader()\n",
    "\n",
    "# Grab a sample\n",
    "LF_img_4D, GT_vol = net.val_loader.dataset.__getitem__(0)\n",
    "# perform reconstruction by calling the network with the image (This calls the forward function in the network)\n",
    "pred_vol = net(LF_img_4D.unsqueeze(0))\n",
    "\n",
    "# Compute 2D lenslet image for display\n",
    "LF_img_lenslet = LF2Spatial(LF_img_4D.unsqueeze(0), LFshape)\n",
    "# Compute volume Maximum intensity projection\n",
    "vol_pred_MIP = volume_2_projections(pred_vol)\n",
    "vol_GT_MIP = volume_2_projections(GT_vol.float().unsqueeze(0))\n",
    "\n",
    "# Show input and output\n",
    "plt.subplot(1,3,1)\n",
    "# Pytorch output dimensions are [batch, channel, x, y]. \n",
    "#We grab only the [x,y] and convert it to cpu and numpy for display\n",
    "plt.imshow(LF_img_lenslet[0,0].cpu().detach().numpy()) \n",
    "plt.title('Input Light-field image')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(vol_GT_MIP[0,0].cpu().detach().numpy()) \n",
    "plt.title('GT 3D volume')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(vol_pred_MIP[0,0].cpu().detach().numpy()) \n",
    "plt.title('Output 3D volume')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "adefa1ab0f692421d76c5f439f08fdb2194ecaa0be285eca5571b46bfcc0e371"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
